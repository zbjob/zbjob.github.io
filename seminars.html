
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
        <script type="application/ld+json">
            {
                "@context": "http://schema.org",
                "@type": "Person",
                "name": "Bo Zhao",
                "url": "https://zbjob.github.io",
                "jobTitle": "Assistant Professor",
                "alumniOf": "Imperial College London",
                "gender": "male",
                "image": "https://amor.cms.hu-berlin.de/~zhaobo/images/zhao.jpg",
                "sameAs": [
                    "https://amor.cms.hu-berlin.de/~zhaobo/",
                    "https://www.linkedin.com/in/bo-zhao-054b7aa8/",
                    "https://twitter.com/bo_zhao_",
                    "https://scholar.google.de/citations?user=R6igYYYAAAAJ&hl=en",
                    "https://www.facebook.com/bo.zhao.779205",
                    "https://github.com/zbjob",
                    "https://www.instagram.com/bo.zhao_real/",
                    "https://zbjob.github.io/",
                    "http://www.eecs.qmul.ac.uk/~bozhao/"
                    ]
            }
        </script>

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Bo Zhao</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Bo Zhao" />
	<meta name="keywords" content="Bo Zhao, Aalto University" />
	<meta name="author" content="Bo Zhao" />

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<!-- <link href='https://fonts.googleapis.com/css?family=Work+Sans:400,300,600,400italic,700' rel='stylesheet' type='text/css'> -->
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body style="background-image: url(images/bg_img.jpg);">
	
	<div id="fh5co-main">
		<div class="fh5co-tab-wrap">
            
            <div class="fh5co-tab-content active" data-content="1">
                <div class="fh5co-content-inner text-left">
                    <div class="row row-bottom-padded-sm">
                        <div class="col-md-12">
                            <p>The <a href="https://zbjob.github.io">Aalto Data-Intensive System group (ADIS) gourp</a> seminars provide a forum to discuss the state of the art in systems research with experts from leading academic institutes and industry labs. All events are free to join, please reach out to our group members for accesses. </p>

                        <h3>Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models</h3>
                             <p><a href="https://wenqijiang.github.io/">Wenqi Jiang</a>, ETH Zürich, 7.Mar.2024, 14:00 EET, Online </p>
                             <p><b>[Abstract]</b>
 The recent advances in generative large language models (LLMs) are attributable to the surging number of model parameters trained on massive datasets.    However, improving LLM quality by scaling up models leads to several major problems including high computational costs. Instead of scaling up the         models, a promising direction, which OpenAI has recently adopted, is known as Retrieval-Augmented Language Model (RALM), which augments a large language  model (LLM) by retrieving context-specific knowledge from an external database via vector search. This strategy facilitates impressive text generation    quality even with smaller models, thus reducing computational demands by orders of magnitude.
                 </p>


                             <p>
 However, RALMs introduce unique system design challenges due to (a) the diverse workload characteristics between LLM inference and retrieval and (b) the  various system requirements and bottlenecks for different RALM configurations including model sizes, database sizes, and retrieval frequencies. In this   talk, I will present Chameleon, a heterogeneous accelerator system integrating both LLM and retrieval accelerators in a disaggregated architecture. The   heterogeneity ensures efficient serving for both LLM inference and retrieval, while the disaggregation allows independent scaling of LLM and retrieval    of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype implements retrieval accelerators on FPGAs and assigns LLM inference to     GPUs, with a CPU server orchestrating these accelerators over the network. Evaluated on various RALMs, Chameleon exhibits up to 2.16× reduction in        latency and 3.18× speedup in throughput compared to the hybrid CPU-GPU architecture.
 </p>

 <p><b>[About the speaker]</b>
 Wenqi Jiang is a fourth-year Ph.D. student at ETH Zurich, where he is affiliated with the systems group advised by Gustavo Alonso and Torsten Hoefler.    Wenqi's research interests span data management, computer architecture, and computer systems. His work primarily focuses on designing post-Moore data     systems, which involve cross-stack solutions including algorithm, system, and architecture innovations. Some examples of his work include large language  models, vector search, recommender systems, and spatial data processing.
 </p>

 <hr style="width:100%;text-align:left;margin-left:0">

 <h3>DeltaZip: Multi-tenant Language Models Serving via Delta Compression</h3>
                             <p><a href="https://about.yao.sh/">Xiaozhe Yao</a>, ETH Zürich, 26.Feb.2024, 14:00 EET, Online </p>
                             <p><b>[Abstract]</b> Fine-tuning large language models (LLMs) for downstream tasks can greatly improve model quality,         however serving many different fine-tuned LLMs concurrently for users in multi-tenant environments is challenging. Dedicating GPU memory for each model   is prohibitively expensive and naively swapping large model weights in and out of GPU memory is slow. Our key insight is that fine-tuned models can be    quickly swapped in and out of GPU memory by extracting and compressing the delta between each model and its pre-trained base model. We propose DeltaZip,  an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by a        factor of 6× to 8× while maintaining high model quality. DeltaZip increases serving throughput by 1.5× to 3× and improves SLO attainment compared to a    vanilla HuggingFace serving system.
                             </p>


 <p><b>[About the speaker]</b> Xiaozhe Yao is a second-year doctoral student at Systems Group, Department of Computer Science, ETH Zürich advised by       Prof. Dr. Ana Klimović. Working on a wide spectrum of machine learning and systems, his research direction is to build systems that support large-scale   machine learning and democratize machine learning. Prior to ETH, Xiaozhe Yao gained his Master’s degree at the University of Zurich in Data Science,      advised by Prof. Dr. Michael Böhlen and Qing Chen. Before that, he completed his Bachelor’s study at Shenzhen University in Computer Science, advised by  Prof. Dr. Shiqi Yu. He interned at Shenzhen Institute of Advanced Technology in 2016 as a data scientist.</p>
 <hr style="width:100%;text-align:left;margin-left:0">

                             <h3>TempoRL: Efficient Deep Reinforcement Learning with Recurrent Tensors</h3>
                             <p><a href="https://www.doc.ic.ac.uk/~pms20/">Pedro Silvestre</a>, Imperial College London, 19.Feb.2024, 13:00 EET, Online </ p>
                             <p><b>[Abstract]</b> Reinforcement Learning (RL) is an increasingly relevant area of algorithmic research. Though RL differs  substantially from Supervised Learning (SL), today's RL frameworks are often simple wrappers over SL systems. In this talk, we first analyse the          differences between SL and RL from the system designer's point-of-view, then discuss the issues and inefficiencies of RL frameworks arising from those    differences. In particular, we discuss how the existence
                             of cyclic and dynamic data dependencies in RL forces the decomposition of algorithms into disjoint dataflow graphs,           preventing holistic analysis and optimisation.</p>

                             <p>We then propose TempoRL, a system designed to efficiently capture these cyclic and dynamic data dependencies in a single   graph by instead viewing RL algorithms as Systems of Recurrence Equations (SREs). TempoRL is then able to holistically analyse and optimise this graph,   applying both classic and novel transformations like automatic vectorisation (when memory allows) or incrementalisation (when memory is scarce).          Because SREs impose no control-flow, TempoRL is free to choose any execution schedule that respects the data dependencies. Luckily, by designing around   SREs, we are able to leverage the powerful polyhedral analysis framework to find efficient and parallel execution schedules, as well as, compute a        memory management plan through dataflow analysis. The remainder of the talk discusses the surprising advantages that this novel computational model       brings, and the applications it may have outside of RL.
 </p>

 <p><b>[About the speaker]</b> Pedro Silvestre is a PhD student in the Large-Scale Data & Systems Group at Imperial College London, under the supervision  of Prof. Peter Pietzuch, working on Dataflow Systems for Deep Reinforcement Learning. Before Imperial, Pedro was a Research Engineer at the TU Delft’s    Web Information Systems Group working on Consistent Fault-tolerance for Distributed Stream Processing. Pedro completed both his MSc and BSc from the      NOVA School of Science and Technology. </p>
						</div>
					</div>
				</div>
			</div>
	</div>
	
	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Easy PieChart -->
	<script src="js/jquery.easypiechart.min.js"></script>
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>

